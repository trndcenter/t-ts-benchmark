save_dataset: False
target: "status"
creator: !AttentionCreator
  inference: False
  without_unstable: False
  path_inference: "inference/attention"
  embedding_dim: 32
  hidden_dim: 8
  output_dim: 1
  heads: 8
  epochs: 3
  batch_size: 16
  lr: 0.0001
  margin: 0.5
  device: "cpu"

transformer: !XGBTransformer
  drop_patterns:
    - "vcs_commit_sha"
    - "allure_id"
    - "test_file_path"
    - "test_method"
    - "unknown_files_count"
    - "test_case_id"
    - "cross_file_"
  file_drop_patterns:
    - "_number_changes_3d"
    - "_number_changes_14d"
    - "_number_changes_56d"
    - "_distinct_authors"
    - "_change_type"
    - "_lines_added"
    - "_lines_deleted"
    - "_changed"
  file_features:
    - "number_changes_3d"
    - "number_changes_14d"
    - "number_changes_56d"
    - "distinct_authors"
    - "change_type"
    - "lines_added"
    - "lines_deleted"
    - "changed"
  preprocessing_files: False
  frequent_files_limit: 6000
  sparse: True

model: !XGBModel
  objective: "binary:logistic"
  max_depth: 3
  learning_rate: 0.07272710602456586
  n_estimators: 300
  min_child_weight: 10
  colsample_bytree: 0.48974814671431044
  subsample: 0.5664090717099939
  reg_alpha: 0.014022071714841626
  reg_lambda: 0.026176884279133304
  scale_pos_weight: 0.2454826013327081
